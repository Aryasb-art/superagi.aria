"""
EthicalReasoningAgent - Ethical reasoning and value alignment analysis agent.

This agent analyzes decisions, strategies, or statements based on ethical reasoning frameworks,
detecting potential ethical misalignments, value conflicts, and integrity risks.
"""

import json
import logging
import re
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any

from sqlalchemy import Column, String, DateTime, Float, Text, create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

from agents.base_agent import BaseAgent
from config import settings
from openai import OpenAI
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

Base = declarative_base()

class EthicalLog(Base):
    """Database model for ethical reasoning logs."""
    __tablename__ = 'ethical_logs'
    
    id = Column(String, primary_key=True)
    timestamp = Column(DateTime, default=datetime.utcnow)
    input_text = Column(Text)
    status = Column(String)  # ok, warning, alert
    confidence = Column(Float)
    framework_flags = Column(Text)  # JSON string of triggered frameworks
    guidance = Column(Text)
    analysis_data = Column(Text)  # JSON string of full analysis


class EthicalReasoningAgent(BaseAgent):
    """
    Advanced ethical reasoning and value alignment analysis agent.
    Analyzes decisions and statements based on major ethical frameworks.
    """
    
    def __init__(self):
        super().__init__("EthicalReasoningAgent")
        self.openai_client = None
        self.session = None
        self._init_database()
        self._init_openai()
        self._init_ethical_frameworks()
        logger.info(f"[{self.name}] Initialized with PostgreSQL database integration for ethical reasoning analysis")
    
    def _init_database(self):
        """Initialize database connection and create tables."""
        try:
            engine = create_engine(settings.DATABASE_URL)
            Base.metadata.create_all(engine)
            Session = sessionmaker(bind=engine)
            self.session = Session()
            logger.info(f"[{self.name}] Ethical logs table ready")
        except Exception as e:
            logger.error(f"[{self.name}] Database initialization failed: {e}")
            self.session = None
    
    def _init_openai(self):
        """Initialize OpenAI client."""
        try:
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                logger.info(f"[{self.name}] OpenAI GPT integration ready for ethical reasoning analysis")
            else:
                logger.warning(f"[{self.name}] OpenAI API key not found, using pattern-based analysis only")
        except Exception as e:
            logger.error(f"[{self.name}] OpenAI initialization failed: {e}")
            self.openai_client = None
    
    def _init_ethical_frameworks(self):
        """Initialize ethical frameworks and pattern detection."""
        # Ethical framework patterns for detection
        self.ethical_patterns = {
            'utilitarianism': {
                'keywords': ['greatest good', 'greatest happiness', 'utility', 'consequence', 'outcome', 'benefit most people', 'maximum welfare', 'cost-benefit'],
                'persian_keywords': ['Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø®ÛŒØ±', 'Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø³ÙˆØ¯', 'Ù†ÙØ¹ Ø¹Ù…ÙˆÙ…ÛŒ', 'Ø±ÙØ§Ù‡ Ø¬Ø§Ù…Ø¹Ù‡', 'Ù†ØªÛŒØ¬Ù‡', 'Ù¾ÛŒØ§Ù…Ø¯', 'Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù…Ù†ÙØ¹Øª'],
                'concerns': ['ends justify means', 'individual sacrifice', 'minority rights ignored']
            },
            'deontology': {
                'keywords': ['duty', 'obligation', 'categorical imperative', 'universal law', 'inherent right', 'moral rule', 'principle'],
                'persian_keywords': ['ÙˆØ¸ÛŒÙÙ‡', 'ØªÚ©Ù„ÛŒÙ', 'Ø§ØµÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ', 'Ù‚Ø§Ù†ÙˆÙ† Ú©Ù„ÛŒ', 'Ø­Ù‚ Ø°Ø§ØªÛŒ', 'Ù‚Ø§Ø¹Ø¯Ù‡ Ø§Ø®Ù„Ø§Ù‚ÛŒ', 'Ø§ØµÙ„'],
                'concerns': ['rigid rules', 'inflexible', 'ignores consequences']
            },
            'virtue_ethics': {
                'keywords': ['character', 'virtue', 'integrity', 'honesty', 'courage', 'compassion', 'wisdom', 'temperance'],
                'persian_keywords': ['Ø´Ø®ØµÛŒØª', 'ÙØ¶ÛŒÙ„Øª', 'ØµØ¯Ø§Ù‚Øª', 'Ø´Ø¬Ø§Ø¹Øª', 'Ø¯Ù„Ø³ÙˆØ²ÛŒ', 'Ø­Ú©Ù…Øª', 'Ø§Ø¹ØªØ¯Ø§Ù„', 'Ø¯Ø±Ø³ØªÚ©Ø§Ø±ÛŒ'],
                'concerns': ['cultural relativism', 'vague guidance', 'subjective virtues']
            },
            'care_ethics': {
                'keywords': ['care', 'relationship', 'responsibility', 'empathy', 'nurturing', 'interdependence', 'context'],
                'persian_keywords': ['Ù…Ø±Ø§Ù‚Ø¨Øª', 'Ø±Ø§Ø¨Ø·Ù‡', 'Ù…Ø³Ø¦ÙˆÙ„ÛŒØª', 'Ù‡Ù…Ø¯Ù„ÛŒ', 'Ù¾Ø±ÙˆØ±Ø´', 'ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ù…ØªÙ‚Ø§Ø¨Ù„', 'Ø¨Ø§ÙØª'],
                'concerns': ['partiality', 'limited scope', 'gender bias']
            }
        }
        
        # Ethical risk indicators
        self.risk_indicators = {
            'high_risk': [
                'harm others', 'deception', 'exploitation', 'discrimination', 'manipulation',
                'violence', 'fraud', 'corruption', 'abuse', 'injustice',
                'Ø¢Ø³ÛŒØ¨ Ø¨Ù‡ Ø¯ÛŒÚ¯Ø±Ø§Ù†', 'ÙØ±ÛŒØ¨', 'Ø¨Ù‡Ø±Ù‡â€ŒÚ©Ø´ÛŒ', 'ØªØ¨Ø¹ÛŒØ¶', 'Ø¯Ø³ØªÚ©Ø§Ø±ÛŒ',
                'Ø®Ø´ÙˆÙ†Øª', 'Ú©Ù„Ø§Ù‡Ø¨Ø±Ø¯Ø§Ø±ÛŒ', 'ÙØ³Ø§Ø¯', 'Ø³ÙˆØ¡Ø§Ø³ØªÙØ§Ø¯Ù‡', 'Ø¨ÛŒâ€ŒØ¹Ø¯Ø§Ù„ØªÛŒ'
            ],
            'medium_risk': [
                'unfair advantage', 'bias', 'conflict of interest', 'privacy violation',
                'breach of trust', 'negligence', 'irresponsibility',
                'Ù…Ø²ÛŒØª Ù†Ø§Ø¹Ø§Ø¯Ù„Ø§Ù†Ù‡', 'ØªØ¹ØµØ¨', 'ØªØ¶Ø§Ø¯ Ù…Ù†Ø§ÙØ¹', 'Ù†Ù‚Ø¶ Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ',
                'Ø´Ú©Ø³Øª Ø§Ø¹ØªÙ…Ø§Ø¯', 'ØºÙÙ„Øª', 'Ø¨ÛŒâ€ŒÙ…Ø³Ø¦ÙˆÙ„ÛŒØªÛŒ'
            ],
            'inaction_risks': [
                'failing to act', 'ignoring', 'negligence', 'turning blind eye',
                'avoiding responsibility', 'passive acceptance',
                'Ø¹Ø¯Ù… Ø§Ù‚Ø¯Ø§Ù…', 'Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ†', 'ØºÙÙ„Øª', 'Ú†Ø´Ù… Ø¨Ø³ØªÙ†',
                'ÙØ±Ø§Ø± Ø§Ø² Ù…Ø³Ø¦ÙˆÙ„ÛŒØª', 'Ù¾Ø°ÛŒØ±Ø´ Ù…Ù†ÙØ¹Ù„Ø§Ù†Ù‡'
            ]
        }
        
        logger.info(f"[{self.name}] Ethical frameworks and risk patterns initialized")
    
    def _detect_ethical_patterns(self, text: str) -> Dict[str, Any]:
        """
        Detect ethical frameworks and risks using pattern matching.
        
        Args:
            text (str): Input text to analyze
            
        Returns:
            Dict: Detected ethical patterns with scores
        """
        text_lower = text.lower()
        
        # Detect ethical frameworks
        framework_scores = {}
        triggered_frameworks = []
        
        for framework, patterns in self.ethical_patterns.items():
            score = 0
            all_keywords = patterns['keywords'] + patterns['persian_keywords']
            
            for keyword in all_keywords:
                if keyword.lower() in text_lower:
                    score += 1
            
            if score > 0:
                framework_scores[framework] = score / len(all_keywords)
                triggered_frameworks.append(framework)
        
        # Detect risk levels
        risk_score = 0.0
        risk_indicators = []
        
        # High risk indicators
        for indicator in self.risk_indicators['high_risk']:
            if indicator.lower() in text_lower:
                risk_score += 0.3
                risk_indicators.append(f"High risk: {indicator}")
        
        # Medium risk indicators
        for indicator in self.risk_indicators['medium_risk']:
            if indicator.lower() in text_lower:
                risk_score += 0.2
                risk_indicators.append(f"Medium risk: {indicator}")
        
        # Inaction risks
        for indicator in self.risk_indicators['inaction_risks']:
            if indicator.lower() in text_lower:
                risk_score += 0.15
                risk_indicators.append(f"Inaction risk: {indicator}")
        
        # Normalize risk score
        risk_score = min(1.0, risk_score)
        
        # Determine status
        if risk_score >= 0.7:
            status = "alert"
        elif risk_score >= 0.4:
            status = "warning"
        else:
            status = "ok"
        
        return {
            'frameworks': triggered_frameworks,
            'framework_scores': framework_scores,
            'risk_score': risk_score,
            'risk_indicators': risk_indicators,
            'status': status,
            'confidence': 0.7 if triggered_frameworks else 0.4,
            'detection_method': 'pattern_based'
        }
    
    def _analyze_ethics_with_gpt(self, text: str) -> Dict[str, Any]:
        """
        Analyze ethical reasoning using OpenAI GPT-4o.
        
        Args:
            text (str): Input text to analyze
            
        Returns:
            Dict: GPT analysis results
        """
        if not self.openai_client:
            return self._detect_ethical_patterns(text)
        
        try:
            # the newest OpenAI model is "gpt-4o" which was released May 13, 2024.
            # do not change this unless explicitly requested by the user
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": """You are an expert ethical reasoning analyst. Analyze the given text for ethical implications based on major ethical frameworks:

1. **Utilitarianism**: Greatest good for greatest number
2. **Deontology**: Duty-based ethics, universal principles
3. **Virtue Ethics**: Character-based ethics, virtues and vices
4. **Care Ethics**: Relationship-based ethics, care and responsibility

Analyze for:
- Ethical frameworks triggered
- Potential value conflicts
- Integrity risks
- Ethical negligence (inaction risks)
- Both actions and inactions

Respond with JSON in this exact format:
{
  "frameworks": ["list of triggered frameworks"],
  "risk_score": 0.0-1.0,
  "status": "ok/warning/alert",
  "confidence": 0.0-1.0,
  "reasoning_summary": "brief analysis in Persian",
  "ethical_guidance": "improvement suggestions in Persian",
  "inaction_risks": ["list of negligence risks"],
  "framework_analysis": {
    "utilitarianism": "brief assessment",
    "deontology": "brief assessment", 
    "virtue_ethics": "brief assessment",
    "care_ethics": "brief assessment"
  }
}"""
                    },
                    {
                        "role": "user",
                        "content": f"Analyze this text for ethical reasoning and potential risks:\n\n{text}"
                    }
                ],
                response_format={"type": "json_object"},
                max_tokens=1000
            )
            
            result = json.loads(response.choices[0].message.content)
            result['detection_method'] = 'gpt_analysis'
            return result
            
        except Exception as e:
            logger.error(f"[{self.name}] GPT analysis failed: {e}")
            return self._detect_ethical_patterns(text)
    
    def _generate_ethical_guidance(self, frameworks: List[str], risk_score: float, 
                                  status: str, inaction_risks: List[str] = None) -> str:
        """
        Generate ethical guidance based on analysis results.
        
        Args:
            frameworks (List[str]): Triggered ethical frameworks
            risk_score (float): Risk score
            status (str): Status level
            inaction_risks (List[str]): Inaction risks detected
            
        Returns:
            str: Ethical guidance
        """
        guidance_parts = []
        
        # Status-based guidance
        if status == "alert":
            guidance_parts.append("âš ï¸ **Ù‡Ø´Ø¯Ø§Ø± Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ**: Ø§ÛŒÙ† ØªØµÙ…ÛŒÙ… ÛŒØ§ Ø§Ù‚Ø¯Ø§Ù… Ø¯Ø§Ø±Ø§ÛŒ Ø®Ø·Ø±Ø§Øª Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø¬Ø¯ÛŒ Ø§Ø³Øª.")
        elif status == "warning":
            guidance_parts.append("ğŸŸ¡ **ØªÙˆØ¬Ù‡ Ø§Ø®Ù„Ø§Ù‚ÛŒ**: Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø§Ø³Øª.")
        else:
            guidance_parts.append("âœ… **ÙˆØ¶Ø¹ÛŒØª Ø§Ø®Ù„Ø§Ù‚ÛŒ Ù…Ø·Ù„ÙˆØ¨**: ØªØ­Ù„ÛŒÙ„ Ø§ÙˆÙ„ÛŒÙ‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø¹Ø¯Ù… Ù…Ø´Ú©Ù„ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø¬Ø¯ÛŒ Ø§Ø³Øª.")
        
        # Framework-specific guidance
        framework_guidance = {
            'utilitarianism': "Ø§Ø² Ù…Ù†Ø¸Ø± ÙØ§ÛŒØ¯Ù‡â€ŒÚ¯Ø±Ø§ÛŒÛŒØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø³ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø§ÙØ±Ø§Ø¯ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ø¯Ø§Ø±Ø¯.",
            'deontology': "Ø§Ø² Ù…Ù†Ø¸Ø± Ø§Ø®Ù„Ø§Ù‚ ÙˆØ¸ÛŒÙÙ‡â€ŒÙ…Ø­ÙˆØ±ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ø¨Ø§ Ø§ØµÙˆÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ú©Ù„ÛŒ Ùˆ ÙˆØ¸Ø§ÛŒÙ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª.",
            'virtue_ethics': "Ø§Ø² Ù…Ù†Ø¸Ø± Ø§Ø®Ù„Ø§Ù‚ ÙØ¶ÛŒÙ„ØªØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ù…Ù†Ø¹Ú©Ø³â€ŒÚ©Ù†Ù†Ø¯Ù‡ ÙØ¶Ø§ÛŒÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ù…Ø§Ù†Ù†Ø¯ ØµØ¯Ø§Ù‚ØªØŒ Ø´Ø¬Ø§Ø¹Øª Ùˆ Ø¹Ø¯Ø§Ù„Øª Ø§Ø³Øª.",
            'care_ethics': "Ø§Ø² Ù…Ù†Ø¸Ø± Ø§Ø®Ù„Ø§Ù‚ Ù…Ø±Ø§Ù‚Ø¨ØªØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ø±ÙˆØ§Ø¨Ø· Ùˆ Ù…Ø³Ø¦ÙˆÙ„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø§Ù‚Ø¨ØªÛŒ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯."
        }
        
        for framework in frameworks:
            if framework in framework_guidance:
                guidance_parts.append(f"ğŸ“š **{framework}**: {framework_guidance[framework]}")
        
        # Risk-based guidance
        if risk_score >= 0.7:
            guidance_parts.append("ğŸ”´ **ØªÙˆØµÛŒÙ‡ ÙÙˆØ±ÛŒ**: Ø§ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ø±Ø§ Ù…ØªÙˆÙ‚Ù Ú©Ø±Ø¯Ù‡ Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯.")
        elif risk_score >= 0.4:
            guidance_parts.append("ğŸŸ  **ØªÙˆØµÛŒÙ‡ Ø§Ø­ØªÛŒØ§Ø·**: Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¯Ø§Ù…Ù‡ØŒ ØªØ¨Ø¹Ø§Øª Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
        
        # Inaction risks
        if inaction_risks:
            guidance_parts.append("âš ï¸ **Ø®Ø·Ø± ØºÙÙ„Øª Ø§Ø®Ù„Ø§Ù‚ÛŒ**: Ø¹Ø¯Ù… Ø§Ù‚Ø¯Ø§Ù… Ù†ÛŒØ² Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø§Ø±Ø§ÛŒ ØªØ¨Ø¹Ø§Øª Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø¨Ø§Ø´Ø¯.")
        
        # General guidance
        guidance_parts.extend([
            "ğŸ¯ **Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¹Ù…Ù„ÛŒ**:",
            "â€¢ Ø¨Ø§ Ø§ÙØ±Ø§Ø¯ Ù…ØªØ£Ø«Ø± Ù…Ø´ÙˆØ±Øª Ú©Ù†ÛŒØ¯",
            "â€¢ ØªØ¨Ø¹Ø§Øª Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ùˆ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯",
            "â€¢ Ø§ØµÙˆÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ùˆ Ø´Ø®ØµÛŒ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯",
            "â€¢ Ø¯Ø± ØµÙˆØ±Øª Ø´Ú©ØŒ Ø§Ø² Ù…Ø´Ø§ÙˆØ±Ø§Ù† Ø§Ø®Ù„Ø§Ù‚ÛŒ Ú©Ù…Ú© Ø¨Ú¯ÛŒØ±ÛŒØ¯"
        ])
        
        return "\n".join(guidance_parts)
    
    def analyze_ethics(self, text: str) -> Dict[str, Any]:
        """
        Analyze text for ethical reasoning using hybrid approach.
        
        Args:
            text (str): Input text to analyze
            
        Returns:
            Dict: Ethical analysis results
        """
        if not text or len(text.strip()) < 10:
            return {
                'success': False,
                'error': 'Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª (Ø­Ø¯Ø§Ù‚Ù„ 10 Ú©Ø§Ø±Ø§Ú©ØªØ±)'
            }
        
        try:
            # Try GPT analysis first, fallback to pattern-based
            if self.openai_client:
                analysis = self._analyze_ethics_with_gpt(text)
            else:
                analysis = self._detect_ethical_patterns(text)
            
            # Generate guidance
            guidance = self._generate_ethical_guidance(
                analysis.get('frameworks', []),
                analysis.get('risk_score', 0.0),
                analysis.get('status', 'ok'),
                analysis.get('inaction_risks', [])
            )
            
            # Prepare result
            result = {
                'status': analysis.get('status', 'ok'),
                'confidence': analysis.get('confidence', 0.5),
                'framework_flags': analysis.get('frameworks', []),
                'risk_score': analysis.get('risk_score', 0.0),
                'reasoning_summary': analysis.get('reasoning_summary', ''),
                'ethical_guidance': guidance,
                'inaction_risks': analysis.get('inaction_risks', []),
                'detection_method': analysis.get('detection_method', 'pattern_based'),
                'framework_analysis': analysis.get('framework_analysis', {}),
                'timestamp': datetime.utcnow().isoformat()
            }
            
            # Save to database
            log_id = str(uuid.uuid4())
            self._save_ethical_log(
                log_id=log_id,
                input_text=text,
                status=result['status'],
                confidence=result['confidence'],
                framework_flags=json.dumps(result['framework_flags']),
                guidance=guidance,
                analysis_data=json.dumps(result)
            )
            
            return {
                'success': True,
                'analysis': result,
                'log_id': log_id
            }
            
        except Exception as e:
            logger.error(f"[{self.name}] Ethical analysis failed: {e}")
            return {
                'success': False,
                'error': f'Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ: {str(e)}'
            }
    
    def _save_ethical_log(self, log_id: str, input_text: str, status: str, 
                         confidence: float, framework_flags: str, guidance: str,
                         analysis_data: str):
        """Save ethical analysis to database."""
        if not self.session:
            return
            
        try:
            log_entry = EthicalLog(
                id=log_id,
                timestamp=datetime.utcnow(),
                input_text=input_text,
                status=status,
                confidence=confidence,
                framework_flags=framework_flags,
                guidance=guidance,
                analysis_data=analysis_data
            )
            self.session.add(log_entry)
            self.session.commit()
        except Exception as e:
            logger.error(f"[{self.name}] Failed to save ethical log: {e}")
            if self.session:
                self.session.rollback()
    
    def get_ethical_logs(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get recent ethical analysis logs.
        
        Args:
            limit (int): Number of logs to retrieve
            
        Returns:
            List[Dict]: Recent ethical logs
        """
        if not self.session:
            return []
            
        try:
            logs = self.session.query(EthicalLog).order_by(
                EthicalLog.timestamp.desc()
            ).limit(limit).all()
            
            return [
                {
                    'id': log.id,
                    'timestamp': log.timestamp.isoformat(),
                    'input_text': log.input_text,
                    'status': log.status,
                    'confidence': log.confidence,
                    'framework_flags': json.loads(log.framework_flags) if log.framework_flags else [],
                    'guidance': log.guidance,
                    'analysis_data': json.loads(log.analysis_data) if log.analysis_data else {}
                }
                for log in logs
            ]
        except Exception as e:
            logger.error(f"[{self.name}] Failed to retrieve ethical logs: {e}")
            return []
    
    def respond(self, message: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Process message and analyze for ethical reasoning.
        
        Args:
            message (str): The message to analyze
            context (Dict): Optional context information
            
        Returns:
            Dict: Response containing ethical analysis
        """
        if not message or not message.strip():
            return {
                'success': False,
                'content': 'Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† ÛŒØ§ ØªØµÙ…ÛŒÙ… Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯'
            }
        
        # Analyze ethics
        analysis_result = self.analyze_ethics(message)
        
        if not analysis_result['success']:
            return {
                'success': False,
                'content': analysis_result['error']
            }
        
        analysis = analysis_result['analysis']
        
        # Format response
        response_parts = []
        
        # Status indicator
        status_emoji = {
            'ok': 'âœ…',
            'warning': 'ğŸŸ¡', 
            'alert': 'ğŸ”´'
        }
        
        response_parts.append(f"{status_emoji.get(analysis['status'], 'â“')} **ÙˆØ¶Ø¹ÛŒØª Ø§Ø®Ù„Ø§Ù‚ÛŒ**: {analysis['status']}")
        
        # Confidence
        confidence_bar = 'â–ˆ' * int(analysis['confidence'] * 5) + 'â–‘' * (5 - int(analysis['confidence'] * 5))
        response_parts.append(f"ğŸ“Š **Ø§Ø·Ù…ÛŒÙ†Ø§Ù†**: {confidence_bar} ({analysis['confidence']*100:.1f}%)")
        
        # Risk score
        if analysis['risk_score'] > 0:
            risk_bar = 'â–ˆ' * int(analysis['risk_score'] * 5) + 'â–‘' * (5 - int(analysis['risk_score'] * 5))
            response_parts.append(f"âš ï¸ **Ø§Ù…ØªÛŒØ§Ø² Ø®Ø·Ø±**: {risk_bar} ({analysis['risk_score']*100:.1f}%)")
        
        # Triggered frameworks
        if analysis['framework_flags']:
            frameworks_persian = {
                'utilitarianism': 'ÙØ§ÛŒØ¯Ù‡â€ŒÚ¯Ø±Ø§ÛŒÛŒ',
                'deontology': 'Ø§Ø®Ù„Ø§Ù‚ ÙˆØ¸ÛŒÙÙ‡â€ŒÙ…Ø­ÙˆØ±',
                'virtue_ethics': 'Ø§Ø®Ù„Ø§Ù‚ ÙØ¶ÛŒÙ„Øª',
                'care_ethics': 'Ø§Ø®Ù„Ø§Ù‚ Ù…Ø±Ø§Ù‚Ø¨Øª'
            }
            framework_list = [frameworks_persian.get(f, f) for f in analysis['framework_flags']]
            response_parts.append(f"ğŸ“š **Ú†Ø§Ø±Ú†ÙˆØ¨â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡**: {', '.join(framework_list)}")
        
        # Reasoning summary
        if analysis.get('reasoning_summary'):
            response_parts.append(f"ğŸ§  **Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„**: {analysis['reasoning_summary']}")
        
        # Inaction risks
        if analysis.get('inaction_risks'):
            response_parts.append(f"âš ï¸ **Ø®Ø·Ø±Ø§Øª ØºÙÙ„Øª**: {len(analysis['inaction_risks'])} Ù…ÙˆØ±Ø¯ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
        
        # Guidance
        response_parts.append(f"\n{analysis['ethical_guidance']}")
        
        # Method used
        method_text = "ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ GPT" if analysis['detection_method'] == 'gpt_analysis' else "ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÛŒÛŒ"
        response_parts.append(f"\nğŸ” **Ø±ÙˆØ´ ØªØ­Ù„ÛŒÙ„**: {method_text}")
        
        return {
            'success': True,
            'content': '\n'.join(response_parts),
            'analysis_data': analysis
        }
"""
Reward Agent for detecting positive progress and providing motivational feedback.
"""

import os
import json
import logging
import re
from datetime import datetime
from typing import Dict, Any, Optional, List
from enum import Enum
from uuid import uuid4
from sqlalchemy import Column, String, Float, DateTime, Text, create_engine
from sqlalchemy.dialects.postgresql import UUID as PGUUID
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.sql import func
from openai import OpenAI
from .base_agent import BaseAgent

# Database setup
Base = declarative_base()

class TriggerType(Enum):
    EMOTIONAL_RECOVERY = "emotional_recovery"
    GOAL_ALIGNMENT = "goal_alignment"
    SECURITY_IMPROVEMENT = "security_improvement"
    STRESS_REDUCTION = "stress_reduction"
    POSITIVE_MINDSET = "positive_mindset"
    BREAKTHROUGH = "breakthrough"
    CONSISTENCY = "consistency"
    NONE = "none"

class RewardLog(Base):
    """Database model for storing reward feedback and logs."""
    __tablename__ = "reward_logs"
    
    id = Column(PGUUID(as_uuid=True), primary_key=True, default=uuid4, index=True)
    timestamp = Column(DateTime(timezone=True), server_default=func.now())
    trigger_type = Column(Text, nullable=False)
    reward_message = Column(Text, nullable=False)
    emoji = Column(Text, nullable=False)
    confidence = Column(Float, nullable=False, default=0.0)

class RewardAgent(BaseAgent):
    """
    Reward Agent for detecting positive progress and providing motivational feedback.
    Automatically triggered by positive outputs from other agents.
    """
    
    def __init__(self):
        super().__init__("RewardAgent")
        self.logger = logging.getLogger("agent.RewardAgent")
        
        # Database setup
        self.db_url = os.getenv("DATABASE_URL")
        self.engine = create_engine(self.db_url)
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
        
        # Create tables
        self._create_reward_logs_table()
        
        # Initialize OpenAI
        self._initialize_openai()
        
        # Initialize reward patterns and triggers
        self._initialize_reward_patterns()
        
        self.logger.info("[RewardAgent] Initialized with PostgreSQL database integration for reward tracking")

    def _create_reward_logs_table(self):
        """Create reward_logs table if it doesn't exist."""
        try:
            Base.metadata.create_all(bind=self.engine)
            self.logger.info("[RewardAgent] Reward logs table ready")
        except Exception as e:
            self.logger.error(f"[RewardAgent] Error creating reward logs table: {e}")

    def _initialize_openai(self):
        """Initialize OpenAI client if API key is available."""
        try:
            openai_key = os.getenv("OPENAI_API_KEY")
            if openai_key:
                self.openai_client = OpenAI(api_key=openai_key)
                self.logger.info("[RewardAgent] OpenAI GPT integration ready for reward generation")
            else:
                self.openai_client = None
                self.logger.warning("[RewardAgent] OpenAI API key not found, using pattern-based rewards only")
        except Exception as e:
            self.openai_client = None
            self.logger.error(f"[RewardAgent] Error initializing OpenAI: {e}")

    def _initialize_reward_patterns(self):
        """Initialize reward detection patterns and motivational messages."""
        self.reward_patterns = {
            TriggerType.EMOTIONAL_RECOVERY: {
                'keywords': ['Ø¨Ù‡ØªØ±', 'Ø¢Ø±Ø§Ù…Ø´', 'Ø®ÙˆØ¨', 'Ù…Ø«Ø¨Øª', 'Ø´Ø§Ø¯', 'Ø®ÙˆØ´Ø­Ø§Ù„', 'better', 'calm', 'positive', 'happy'],
                'phrases': ['Ø­Ø§Ù„Ù… Ø¨Ù‡ØªØ± Ø´Ø¯', 'Ø¢Ø±Ø§Ù…Ø´ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù…', 'Ø§Ø­Ø³Ø§Ø³ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ù…', 'Ù…Ø«Ø¨Øª ÙÚ©Ø± Ù…ÛŒ Ú©Ù†Ù…'],
                'emojis': ['ðŸŒŸ', 'ðŸ’š', 'ðŸŒˆ', 'ðŸ˜Š', 'ðŸŽ‰'],
                'messages': [
                    'Ø¹Ø§Ù„ÛŒ! Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ø§Ø·ÙÛŒ Ø´Ù…Ø§ Ù‚Ø§Ø¨Ù„ ØªØ­Ø³ÛŒÙ† Ø§Ø³Øª',
                    'Ù¾ÛŒØ´Ø±ÙØª Ø´Ù…Ø§ Ø¯Ø± Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø§Ø³Øª',
                    'Ø±ÙˆØ­ÛŒÙ‡ Ù…Ø«Ø¨Øª Ø´Ù…Ø§ Ø§Ù„Ù‡Ø§Ù…â€ŒØ¨Ø®Ø´ Ø§Ø³Øª',
                    'ØªØ¨Ø±ÛŒÚ©! Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ø§Ø² Ú†Ø§Ù„Ø´ Ø¹Ø§Ø·ÙÛŒ Ø¹Ø¨ÙˆØ± Ú©Ø±Ø¯ÛŒØ¯'
                ]
            },
            TriggerType.GOAL_ALIGNMENT: {
                'keywords': ['Ù‡Ø¯Ù', 'Ù…ÙˆÙÙ‚ÛŒØª', 'Ù¾ÛŒØ´Ø±ÙØª', 'ØªØ­Ù‚Ù‚', 'Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ', 'goal', 'success', 'achievement', 'progress'],
                'phrases': ['Ù‡Ø¯ÙÙ… Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ø±Ø¯Ù…', 'Ù…ÙˆÙÙ‚ Ø´Ø¯Ù…', 'Ù¾ÛŒØ´Ø±ÙØª Ú©Ø±Ø¯Ù…', 'Ø¨Ù‡ Ù‡Ø¯ÙÙ… Ù†Ø²Ø¯ÛŒÚ© Ø´Ø¯Ù…'],
                'emojis': ['ðŸŽ¯', 'ðŸ†', 'â­', 'ðŸš€', 'ðŸ’ª'],
                'messages': [
                    'Ø¨Ø³ÛŒØ§Ø± Ø¹Ø§Ù„ÛŒ! Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø§Ù‡Ø¯Ø§ÙØªØ§Ù† ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø§Ø³Øª',
                    'ØªØ¨Ø±ÛŒÚ©! Ù…Ø³ÛŒØ± Ø¯Ø±Ø³ØªÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯',
                    'Ù¾ÛŒØ´Ø±ÙØª Ø´Ù…Ø§ Ø¯Ø± Ø¬Ù‡Øª Ø§Ù‡Ø¯Ø§Ù Ù‚Ø§Ø¨Ù„ Ø³ØªØ§ÛŒØ´ Ø§Ø³Øª',
                    'Ø¹Ø§Ù„ÛŒ! Ø§Ù†Ú¯ÛŒØ²Ù‡ Ùˆ ØªÙ…Ø±Ú©Ø² Ø´Ù…Ø§ Ø¨Ø± Ø§Ù‡Ø¯Ø§Ù Ú†Ø´Ù…Ú¯ÛŒØ± Ø§Ø³Øª'
                ]
            },
            TriggerType.SECURITY_IMPROVEMENT: {
                'keywords': ['Ø§Ù…Ù†', 'Ù…Ø­Ø§ÙØ¸Øª', 'Ø¨Ù‡Ø¨ÙˆØ¯', 'Ú©Ù†ØªØ±Ù„', 'Ù…Ø¯ÛŒØ±ÛŒØª', 'safe', 'secure', 'improvement', 'control'],
                'phrases': ['Ø§Ø­Ø³Ø§Ø³ Ø§Ù…Ù†ÛŒØª Ù…ÛŒ Ú©Ù†Ù…', 'ÙˆØ¶Ø¹ÛŒØª ØªØ­Øª Ú©Ù†ØªØ±Ù„ Ø§Ø³Øª', 'Ø¨Ù‡ØªØ± Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒ Ú©Ù†Ù…'],
                'emojis': ['ðŸ›¡ï¸', 'ðŸ”’', 'âœ…', 'ðŸŽ–ï¸', 'ðŸ’Ž'],
                'messages': [
                    'Ø¹Ø§Ù„ÛŒ! Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ù…Ù†ÛŒØª Ø°Ù‡Ù†ÛŒ Ø´Ù…Ø§ Ù‚Ø§Ø¨Ù„ ØªÙ‚Ø¯ÛŒØ± Ø§Ø³Øª',
                    'ØªØ¨Ø±ÛŒÚ©! Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ø®Ø·Ø±Ø§Øª Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯',
                    'Ù¾ÛŒØ´Ø±ÙØª Ø´Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ…Ù†â€ŒØ³Ø§Ø²ÛŒ ÙÚ©Ø± ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø§Ø³Øª',
                    'Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨! Ú©Ù†ØªØ±Ù„ Ø¨Ù‡ØªØ±ÛŒ Ø¨Ø± ÙˆØ¶Ø¹ÛŒØª Ø¯Ø§Ø±ÛŒØ¯'
                ]
            },
            TriggerType.STRESS_REDUCTION: {
                'keywords': ['Ø¢Ø±Ø§Ù…', 'Ú©Ø§Ù‡Ø´', 'ØªÙ†Ø´', 'Ø§Ø³ØªØ±Ø§Ø­Øª', 'Ø±ÛŒÙ„Ú©Ø³', 'calm', 'relax', 'reduce', 'peace'],
                'phrases': ['Ø§Ø³ØªØ±Ø³Ù… Ú©Ù… Ø´Ø¯', 'Ø¢Ø±Ø§Ù… Ø´Ø¯Ù…', 'ØªÙ†Ø´Ù… Ú©Ø§Ù‡Ø´ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯', 'Ø§Ø­Ø³Ø§Ø³ Ø¢Ø±Ø§Ù…Ø´ Ù…ÛŒ Ú©Ù†Ù…'],
                'emojis': ['ðŸ•¯ï¸', 'ðŸ§˜', 'ðŸŒ¸', 'â˜®ï¸', 'ðŸƒ'],
                'messages': [
                    'Ø¹Ø§Ù„ÛŒ! Ú©Ø§Ù‡Ø´ Ø§Ø³ØªØ±Ø³ Ø´Ù…Ø§ Ù‚Ø§Ø¨Ù„ Ø³ØªØ§ÛŒØ´ Ø§Ø³Øª',
                    'ØªØ¨Ø±ÛŒÚ©! Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ ØªÙ†Ø´ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯',
                    'Ø¢Ø±Ø§Ù…Ø´ ÛŒØ§ÙØªÙ‡ Ø´Ù…Ø§ Ø§Ù„Ù‡Ø§Ù…â€ŒØ¨Ø®Ø´ Ø§Ø³Øª',
                    'Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨! ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¢Ø±Ø§Ù…Ø´â€ŒØ¨Ø®Ø´ Ø¹Ø§Ù„ÛŒ Ú©Ø§Ø± Ú©Ø±Ø¯Ù‡'
                ]
            },
            TriggerType.POSITIVE_MINDSET: {
                'keywords': ['Ù…Ø«Ø¨Øª', 'Ø§Ù…ÛŒØ¯ÙˆØ§Ø±', 'Ø®ÙˆØ´Ø¨ÛŒÙ†', 'Ø§Ù†Ú¯ÛŒØ²Ù‡', 'Ø§Ù†Ø±Ú˜ÛŒ', 'positive', 'optimistic', 'motivated'],
                'phrases': ['Ù…Ø«Ø¨Øª ÙÚ©Ø± Ù…ÛŒ Ú©Ù†Ù…', 'Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù…', 'Ø§Ù†Ú¯ÛŒØ²Ù‡ Ø¯Ø§Ø±Ù…', 'Ø§Ù†Ø±Ú˜ÛŒ Ù…Ø«Ø¨Øª'],
                'emojis': ['â˜€ï¸', 'ðŸŒ»', 'ðŸ’›', 'ðŸŽˆ', 'ðŸ”¥'],
                'messages': [
                    'ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡! ØªÙÚ©Ø± Ù…Ø«Ø¨Øª Ø´Ù…Ø§ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø§Ø³Øª',
                    'Ø¹Ø§Ù„ÛŒ! Ù†Ú¯Ø±Ø´ Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ø§Ù†Ù‡ Ø´Ù…Ø§ Ù‚Ø§Ø¨Ù„ ØªØ­Ø³ÛŒÙ† Ø§Ø³Øª',
                    'ØªØ¨Ø±ÛŒÚ©! Ø§Ù†Ø±Ú˜ÛŒ Ù…Ø«Ø¨Øª Ø´Ù…Ø§ Ø§Ù„Ù‡Ø§Ù…â€ŒØ¨Ø®Ø´ Ø§Ø³Øª',
                    'Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨! Ø°Ù‡Ù†ÛŒØª Ù…Ø«Ø¨Øª Ø´Ù…Ø§ Ø¯Ø±Ø®Ø´Ø§Ù† Ø§Ø³Øª'
                ]
            },
            TriggerType.BREAKTHROUGH: {
                'keywords': ['Ú©Ø´Ù', 'Ø¯Ø±Ú©', 'ÙÙ‡Ù…ÛŒØ¯Ù†', 'Ø±ÙˆØ´Ù†', 'breakthrough', 'insight', 'understanding'],
                'phrases': ['ÙÙ‡Ù…ÛŒØ¯Ù…', 'Ø±ÙˆØ´Ù† Ø´Ø¯', 'Ú©Ø´Ù Ú©Ø±Ø¯Ù…', 'Ø¯Ø±Ú© Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù…'],
                'emojis': ['ðŸ’¡', 'ðŸ”', 'ðŸŽŠ', 'ðŸŒŸ', 'âš¡'],
                'messages': [
                    'ÙˆØ§Ùˆ! Ú©Ø´Ù Ø´Ù…Ø§ ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø§Ø³Øª',
                    'Ø¹Ø§Ù„ÛŒ! Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚ Ø´Ù…Ø§ Ù‚Ø§Ø¨Ù„ ØªÙ‚Ø¯ÛŒØ± Ø§Ø³Øª',
                    'ØªØ¨Ø±ÛŒÚ©! Ø¨ÛŒÙ†Ø´ Ø¬Ø¯ÛŒØ¯ Ø´Ù…Ø§ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø§Ø³Øª',
                    'Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨! Ø±ÙˆØ´Ù†â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ù…Ø§ Ø§Ù„Ù‡Ø§Ù…â€ŒØ¨Ø®Ø´ Ø§Ø³Øª'
                ]
            }
        }
        
        self.logger.info("[RewardAgent] Reward patterns and motivational messages initialized")

    def respond(self, message: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Analyze input for positive progress and generate reward feedback.
        
        Args:
            message (str): The message to analyze for positive patterns
            context (Dict): Optional context including analysis results from other agents
            
        Returns:
            Dict: Response containing reward feedback if positive progress detected
        """
        try:
            self.remember(f"Reward analysis for: {message[:50]}...")
            
            # Analyze for positive progress patterns
            reward_result = self._analyze_positive_progress(message, context or {})
            
            if reward_result['trigger_type'] != TriggerType.NONE:
                # Save reward log to database
                reward_id = self._save_reward_log(
                    trigger_type=reward_result['trigger_type'].value,
                    reward_message=reward_result['reward_message'],
                    emoji=reward_result['emoji'],
                    confidence=reward_result['confidence']
                )
                
                # Format response
                response_content = self._format_reward_response(reward_result, reward_id)
                
                # Remember the reward
                self.remember(f"Reward generated: {reward_result['trigger_type'].value} - {reward_result['emoji']}")
                
                return {
                    'response_id': f"reward_{reward_id}",
                    'content': response_content,
                    'handled_by': self.name,
                    'timestamp': self._get_current_timestamp(),
                    'success': True,
                    'error': None,
                    'reward_data': {
                        'reward_message': reward_result['reward_message'],
                        'emoji': reward_result['emoji'],
                        'confidence': reward_result['confidence'],
                        'trigger_type': reward_result['trigger_type'].value
                    }
                }
            else:
                # No reward triggered
                return {
                    'response_id': f"no_reward_{self._get_current_timestamp()}",
                    'content': 'Ù‡ÛŒÚ† Ù¾ÛŒØ´Ø±ÙØª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯',
                    'handled_by': self.name,
                    'timestamp': self._get_current_timestamp(),
                    'success': True,
                    'error': None,
                    'reward_data': None
                }
            
        except Exception as e:
            self.logger.error(f"[RewardAgent] Error in respond: {e}")
            return {
                'response_id': f"reward_error_{self._get_current_timestamp()}",
                'content': f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØª: {str(e)}",
                'handled_by': self.name,
                'timestamp': self._get_current_timestamp(),
                'success': False,
                'error': str(e)
            }

    def _analyze_positive_progress(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze text for positive progress patterns."""
        try:
            # First try pattern-based analysis
            pattern_result = self._pattern_based_reward_detection(message, context)
            
            # If OpenAI is available and pattern confidence is low, use GPT analysis
            if self.openai_client and pattern_result['confidence'] < 0.7:
                try:
                    gpt_result = self._gpt_reward_analysis(message, context)
                    # Use GPT result if it has higher confidence
                    if gpt_result['confidence'] > pattern_result['confidence']:
                        return gpt_result
                except Exception as e:
                    self.logger.warning(f"[RewardAgent] GPT analysis failed: {e}")
            
            return pattern_result
            
        except Exception as e:
            self.logger.error(f"[RewardAgent] Error in progress analysis: {e}")
            return {
                'trigger_type': TriggerType.NONE,
                'reward_message': 'ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØª Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯',
                'emoji': 'â“',
                'confidence': 0.0
            }

    def _pattern_based_reward_detection(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Detect positive patterns using keyword and phrase matching."""
        message_lower = message.lower()
        detected_triggers = []
        
        # Check context for positive indicators from other agents
        context_score = self._analyze_context_indicators(context)
        
        for trigger_type, patterns in self.reward_patterns.items():
            trigger_score = 0.0
            matches = []
            
            # Check keywords
            for keyword in patterns['keywords']:
                if keyword in message_lower:
                    trigger_score += 0.3
                    matches.append(f"keyword: {keyword}")
            
            # Check phrases
            for phrase in patterns['phrases']:
                if phrase in message_lower:
                    trigger_score += 0.5
                    matches.append(f"phrase: {phrase}")
            
            # Add context score
            trigger_score += context_score
            
            if trigger_score > 0:
                detected_triggers.append({
                    'type': trigger_type,
                    'score': min(trigger_score, 1.0),
                    'matches': matches,
                    'patterns': patterns
                })
        
        # Select the best trigger
        if detected_triggers:
            best_trigger = max(detected_triggers, key=lambda x: x['score'])
            confidence = min(best_trigger['score'], 0.9)
            
            # Select random message and emoji
            import random
            selected_emoji = random.choice(best_trigger['patterns']['emojis'])
            selected_message = random.choice(best_trigger['patterns']['messages'])
            
            return {
                'trigger_type': best_trigger['type'],
                'reward_message': selected_message,
                'emoji': selected_emoji,
                'confidence': confidence
            }
        
        # No positive progress detected
        return {
            'trigger_type': TriggerType.NONE,
            'reward_message': 'Ù‡ÛŒÚ† Ù¾ÛŒØ´Ø±ÙØª Ù…Ø«Ø¨ØªÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯',
            'emoji': 'â“',
            'confidence': 0.0
        }

    def _analyze_context_indicators(self, context: Dict[str, Any]) -> float:
        """Analyze context for positive indicators from other agents."""
        context_score = 0.0
        
        # Check emotion regulation results
        if 'emotion_result' in context:
            emotion = context['emotion_result'].get('emotion', '').lower()
            if any(pos in emotion for pos in ['Ø®ÙˆØ´Ø­Ø§Ù„', 'Ø¢Ø±Ø§Ù…', 'Ù…Ø«Ø¨Øª', 'happy', 'calm', 'positive']):
                context_score += 0.3
        
        # Check security check results
        if 'security_result' in context:
            alert_level = context['security_result'].get('alert_level', '').lower()
            if alert_level == 'green':
                context_score += 0.2
        
        # Check goal inference results
        if 'goal_result' in context:
            confidence = context['goal_result'].get('confidence', 0)
            if confidence > 0.7:
                context_score += 0.2
        
        return min(context_score, 0.5)

    def _gpt_reward_analysis(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Use OpenAI GPT for advanced reward analysis."""
        try:
            system_prompt = """ØªÙˆ ÛŒÚ© Ù…ØªØ®ØµØµ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ùˆ Ø±ÙˆØ§Ù†â€ŒØ´Ù†Ø§Ø³ Ù…Ø«Ø¨Øª Ù‡Ø³ØªÛŒ. Ù…ØªÙ† Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù† Ùˆ Ø¨Ø¨ÛŒÙ† Ø¢ÛŒØ§ Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù¾ÛŒØ´Ø±ÙØª Ù…Ø«Ø¨ØªØŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø­Ø³Ø§Ø³Ø§ØªØŒ ÛŒØ§ Ø¯Ø³ØªØ§ÙˆØ±Ø¯ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.

Ø§Ù†ÙˆØ§Ø¹ Ù¾ÛŒØ´Ø±ÙØª Ù…Ø«Ø¨Øª:
- emotional_recovery: Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ø§Ø·ÙÛŒ
- goal_alignment: Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø§Ù‡Ø¯Ø§Ù
- security_improvement: Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ù…Ù†ÛŒØª Ø°Ù‡Ù†ÛŒ
- stress_reduction: Ú©Ø§Ù‡Ø´ Ø§Ø³ØªØ±Ø³
- positive_mindset: ØªÙÚ©Ø± Ù…Ø«Ø¨Øª
- breakthrough: Ú©Ø´Ù ÛŒØ§ Ø¯Ø±Ú© Ø¬Ø¯ÛŒØ¯
- consistency: Ø«Ø¨Ø§Øª Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ
- none: Ø¨Ø¯ÙˆÙ† Ù¾ÛŒØ´Ø±ÙØª Ù…Ø«Ø¨Øª

Ø§Ú¯Ø± Ù¾ÛŒØ´Ø±ÙØª Ù…Ø«Ø¨ØªÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯ÛŒØŒ Ù¾ÛŒØ§Ù… ØªØ´ÙˆÛŒÙ‚ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ùˆ ÛŒÚ© Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†.

Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¯Ø± ÙØ±Ù…Øª JSON Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†:
{
  "trigger_type": "Ù†ÙˆØ¹ Ù¾ÛŒØ´Ø±ÙØª",
  "reward_message": "Ù¾ÛŒØ§Ù… ØªØ´ÙˆÛŒÙ‚ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ",
  "emoji": "Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù…Ù†Ø§Ø³Ø¨",
  "confidence": Ø§Ø·Ù…ÛŒÙ†Ø§Ù† (0.0-1.0)
}"""

            response = self.openai_client.chat.completions.create(
                model="gpt-4o",  # the newest OpenAI model is "gpt-4o" which was released May 13, 2024. do not change this unless explicitly requested by the user
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØª Ù…Ø«Ø¨Øª: {message}"}
                ],
                response_format={"type": "json_object"},
                max_tokens=400,
                temperature=0.7
            )
            
            result = json.loads(response.choices[0].message.content)
            
            return {
                'trigger_type': TriggerType(result.get('trigger_type', 'none')),
                'reward_message': result.get('reward_message', 'Ù¾ÛŒØ´Ø±ÙØª Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø´ØªÙ‡â€ŒØ§ÛŒØ¯'),
                'emoji': result.get('emoji', 'ðŸŒŸ'),
                'confidence': float(result.get('confidence', 0.5))
            }
            
        except Exception as e:
            self.logger.error(f"[RewardAgent] GPT analysis failed: {e}")
            raise e

    def _save_reward_log(self, trigger_type: str, reward_message: str, emoji: str, confidence: float) -> str:
        """Save reward log to database."""
        try:
            db = self.SessionLocal()
            
            reward_log = RewardLog(
                trigger_type=trigger_type,
                reward_message=reward_message,
                emoji=emoji,
                confidence=confidence
            )
            
            db.add(reward_log)
            db.commit()
            db.refresh(reward_log)
            
            reward_id = str(reward_log.id)
            db.close()
            
            self.logger.info(f"[RewardAgent] Reward log saved with ID: {reward_id}")
            return reward_id
            
        except Exception as e:
            self.logger.error(f"[RewardAgent] Error saving reward log: {e}")
            return "error"

    def _format_reward_response(self, reward_result: Dict[str, Any], reward_id: str) -> str:
        """Format reward response for display."""
        trigger_type = reward_result['trigger_type']
        reward_message = reward_result['reward_message']
        emoji = reward_result['emoji']
        confidence = reward_result['confidence']
        
        # Get trigger type in Persian
        trigger_persian = {
            TriggerType.EMOTIONAL_RECOVERY: 'Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ø§Ø·ÙÛŒ',
            TriggerType.GOAL_ALIGNMENT: 'Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø§Ù‡Ø¯Ø§Ù',
            TriggerType.SECURITY_IMPROVEMENT: 'Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ù…Ù†ÛŒØª Ø°Ù‡Ù†ÛŒ',
            TriggerType.STRESS_REDUCTION: 'Ú©Ø§Ù‡Ø´ Ø§Ø³ØªØ±Ø³',
            TriggerType.POSITIVE_MINDSET: 'ØªÙÚ©Ø± Ù…Ø«Ø¨Øª',
            TriggerType.BREAKTHROUGH: 'Ú©Ø´Ù Ø¬Ø¯ÛŒØ¯',
            TriggerType.CONSISTENCY: 'Ø«Ø¨Ø§Øª Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ',
            TriggerType.NONE: 'Ù¾ÛŒØ´Ø±ÙØª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯'
        }
        
        response = f"""ðŸ† **Ù¾Ø§Ø¯Ø§Ø´ Ù¾ÛŒØ´Ø±ÙØª:**

```json
{{
  "trigger_type": "{trigger_type.value}",
  "reward_message": "{reward_message}",
  "emoji": "{emoji}",
  "confidence": {confidence:.1f}
}}
```

**ðŸŽ‰ ØªØ¨Ø±ÛŒÚ©!**
{emoji} **{reward_message}**

**ðŸ“Š Ø¬Ø²Ø¦ÛŒØ§Øª Ù¾Ø§Ø¯Ø§Ø´:**
â€¢ ðŸŽ¯ **Ù†ÙˆØ¹ Ù¾ÛŒØ´Ø±ÙØª:** {trigger_persian[trigger_type]}
â€¢ ðŸ“ˆ **Ø§Ø·Ù…ÛŒÙ†Ø§Ù†:** {'â–ˆ' * int(confidence * 5)}{'â–‘' * (5 - int(confidence * 5))} ({confidence * 100:.1f}%)
â€¢ ðŸ†” **Ø´Ù†Ø§Ø³Ù‡ Ù¾Ø§Ø¯Ø§Ø´:** {reward_id[:8]}...

ðŸŒŸ **Ù¾ÛŒØ§Ù… Ø§Ù†Ú¯ÛŒØ²Ø´ÛŒ:** Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ù‡ÛŒØ¯! Ø´Ù…Ø§ Ø¯Ø± Ø­Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØª Ù‡Ø³ØªÛŒØ¯ Ùˆ Ø§ÛŒÙ† Ø¨Ø³ÛŒØ§Ø± Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ø§Ø³Øª."""

        return response

    def _get_current_timestamp(self) -> str:
        """Get current timestamp in ISO format."""
        return datetime.now().isoformat()

    def get_capabilities(self) -> list:
        """Get list of agent capabilities."""
        return [
            "ØªØ´Ø®ÛŒØµ Ù¾ÛŒØ´Ø±ÙØª Ù…Ø«Ø¨Øª",
            "ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØªØ´ÙˆÛŒÙ‚ÛŒ",
            "Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø¯Ø³ØªØ§ÙˆØ±Ø¯Ù‡Ø§",
            "Ø§Ù†Ú¯ÛŒØ²Ù‡â€ŒØ¨Ø®Ø´ÛŒ",
            "Ù¾Ø§Ø¯Ø§Ø´â€ŒØ¯Ù‡ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯",
            "ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯",
            "Ø§ÛŒØ¬Ø§Ø¯ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ù…Ø«Ø¨Øª"
        ]

    def get_reward_statistics(self) -> Dict[str, Any]:
        """Get reward statistics from database."""
        try:
            db = self.SessionLocal()
            
            total_rewards = db.query(RewardLog).count()
            
            # Count by trigger type
            trigger_counts = {}
            for trigger_type in TriggerType:
                if trigger_type != TriggerType.NONE:
                    count = db.query(RewardLog).filter(RewardLog.trigger_type == trigger_type.value).count()
                    trigger_counts[trigger_type.value] = count
            
            # Get average confidence
            avg_confidence = db.query(func.avg(RewardLog.confidence)).scalar() or 0.0
            
            db.close()
            
            return {
                'total_rewards': total_rewards,
                'trigger_types': trigger_counts,
                'average_confidence': round(float(avg_confidence), 2),
                'timestamp': self._get_current_timestamp()
            }
            
        except Exception as e:
            self.logger.error(f"[RewardAgent] Error getting statistics: {e}")
            return {'error': str(e)}

    def list_recent_rewards(self, limit: int = 10) -> Dict[str, Any]:
        """Get recent reward logs."""
        try:
            db = self.SessionLocal()
            
            rewards = db.query(RewardLog).order_by(
                RewardLog.timestamp.desc()
            ).limit(limit).all()
            
            rewards_data = []
            for reward in rewards:
                rewards_data.append({
                    'id': str(reward.id),
                    'timestamp': reward.timestamp.isoformat(),
                    'trigger_type': reward.trigger_type,
                    'reward_message': reward.reward_message,
                    'emoji': reward.emoji,
                    'confidence': reward.confidence
                })
            
            db.close()
            
            return {
                'rewards': rewards_data,
                'count': len(rewards_data),
                'timestamp': self._get_current_timestamp()
            }
            
        except Exception as e:
            self.logger.error(f"[RewardAgent] Error listing rewards: {e}")
            return {'error': str(e), 'rewards': [], 'count': 0}